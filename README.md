# END
This repository contains the assignments submitted as a part of END program from The School of AI. (https://theschoolof.ai/#programs)
![Course Content](https://github.com/vjsurampudi/END/blob/main/image.png?raw=true)

## Session 1 (16-Oct-2020): Background and Basics of NLP 
1. Natural Language Processing
2. Applications of NLP
3. Evolution of NLP
4. Modern NLP
5. NLP Project Pipeline
6. Basics of NLP for text - Coding Section
## Session 2: (23-Oct-2020): From Embeddings to Language Models
1. Embeddings - An analogy from Visual Domain
2. Embeddings
3. Neural Networks
4. Heavy Math - Forward Propagation
5. Backpropagation
6. Word Embeddings
7. Modern Embeddings
8. Word Embeddings vs Language Model
9. Word2Vec
## Session 3: (30-Oct-2020): Advanced Python for General Computing & NLP
1. Quick Refresher
2. While - break, continue and try
3. Objects and Memory References
4. Object Mutability
5. Functional Parameters (*args & **kwargs)
6. Lambda Expressions
7. Map, Filter, Zip and List Comprehension
8. Tuples and Dictionaries and how to unpack them
9. TimeIt

## Session 4: PyTorch
1. Introduction to PyTorch
2. PyTorch vs TensorFlow
3. Tensors
4. AutoGrad
5. Tensors and Numpy
6. Sharing Memory for Performance
7. Tensor Indexing
9. Squeezing and Unsqueezing and other Operations
10.Writing a NN from scratch in Pytorch:

## Session 5: RNNs are dead and their renewed relevance 1
1. Sequential Data
2. Fully Connected Layers
3. Recurrent Neural Networks
4. RNN Cells
5. Vanishing Gradients
6. Training RNNs
7. LSTMs
8. Transitioning from RNNs to LSTMs
    - The tanh - Squashing Function
    - Adding Memory to our recursion cells
    - LSTM Cell
9. LSTM Core Concepts
    - Sigmoid Gate
    - Forget Gate
    - Input Gate
    - Cell State
    - Output Gate
    
 ## Session 6: GRUs, Seq2Se2 & Attention Mechanism
1. RNN/LSTM Recap
    - TanH - Squashing Function
    - Sigmoid - Selection Gate
2. LSTMs
    - LSTM Internals
3. LSTM Variants
4. GRU
5. Encoder=Decoder Architecture in RNN/LSTMs
6. RNN/LSTM with an Attention Mechanism
    - The Context Vector
    - Computing Attention Weights and Context Vectors

## Session 7: Handson - I
## Session 8: Handson - II
## Session 9: Handson - III
 
