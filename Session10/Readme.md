<h2>Neural Machine Translation</h2>

Dataset used - Multi30K

DL architecture used: Encoder Decoder model using GRU and Attention. 

Model 1 uses fixed length of sentences
Model 2 uses mask of tensor in equal length
